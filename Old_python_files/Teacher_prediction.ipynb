{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1oUG3pVSoDyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd91e9ff-c58c-4af4-fb69-d23845ad5d13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd ./drive/MyDrive/Warsztaty badawcze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZUhBChjVeOG",
        "outputId": "23e15374-fdbe-4b21-9e61-acae05e72bef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Warsztaty badawcze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cWcbt02BAloU"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import copy\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from dataset import IDRiD_Dataset\n",
        "from dataset_techers import IDRiD_Dataset_teacher\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pwd"
      ],
      "metadata": {
        "id": "Wd-75vz7VmhD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u-LbM-RsAlof"
      },
      "outputs": [],
      "source": [
        "class MTL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MTL, self).__init__()\n",
        "        resnet50 = models.resnet50(pretrained=True)\n",
        "        self.features = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
        "        self.last = nn.Sequential(nn.Linear(2048, 1024),nn.ReLU())\n",
        "        self.retinopathy_classifier = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 5))\n",
        "        self.macular_edema_classifier = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 5))\n",
        "        self.fovea_center_cords = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 2))\n",
        "        self.optical_disk_cords = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 2))\n",
        "        self.float()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        out = self.features.forward(data).squeeze()\n",
        "        out = self.last.forward(out)\n",
        "        return (self.retinopathy_classifier(out),\n",
        "                self.macular_edema_classifier(out),\n",
        "                self.fovea_center_cords(out),\n",
        "                self.optical_disk_cords(out))\n",
        "\n",
        "    def fit(self, train_dl, optimizer, scheduler, criterion, tasks, epochs):\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.criterion = criterion\n",
        "        self.criterion2 = nn.MSELoss()\n",
        "        self.tasks = tasks\n",
        "        self.epochs = epochs\n",
        "        best_loss=float(\"Inf\")\n",
        "        for e in range(self.epochs):\n",
        "            self.train()\n",
        "            print(\"Epoch: {} \".format(e+1),end=\"\")\n",
        "            current_loss=self.fit_iter(train_dl)\n",
        "            if current_loss.item() < best_loss:\n",
        "                best_loss = current_loss.item()\n",
        "                torch.save(self.state_dict(),'/content/drive/MyDrive/Warsztaty badawcze/weights_teacher.pt')\n",
        "                print(\"Saved best model weights!\")\n",
        "\n",
        "    def fit_iter(self, train_dl):\n",
        "        train_loss = 0.0\n",
        "        loss0sum = 0.0\n",
        "        loss1sum = 0.0\n",
        "        loss2sum = 0.0\n",
        "        loss3sum = 0.0\n",
        "        loss = torch.tensor(0)\n",
        "        accuracy0 = 0.0\n",
        "        accuracy1 = 0.0\n",
        "        for i, (imgs, retinopathy_label, macular_edema_label, fovea_center_labels, optical_disk_labels) in enumerate(train_dl):\n",
        "            fovea_center_labels[:0], fovea_center_labels[:1] = fovea_center_labels[:0]*Rx, fovea_center_labels[:1]*Ry\n",
        "            optical_disk_labels[:0], optical_disk_labels[:1] = optical_disk_labels[:0]*Rx, optical_disk_labels[:1]*Ry\n",
        "            batch_size = imgs.size(0)\n",
        "            self.optimizer.zero_grad()\n",
        "            retinopathy_pred, macular_edema_pred, fovea_center_pred, optical_disk_pred = self.forward(imgs.to(device))\n",
        "            #loss0 = self.criterion(retinopathy_pred, retinopathy_label.to(device).to(torch.int64)).to(torch.float64)*10\n",
        "            #loss1 = self.criterion(macular_edema_pred, macular_edema_label.to(device).to(torch.int64)).to(torch.float64)*10\n",
        "            #loss2 = torch.sqrt(self.criterion2(fovea_center_pred.to(torch.double),fovea_center_labels.to(device).to(torch.double)))/100\n",
        "            #loss3 = torch.sqrt(self.criterion2(optical_disk_pred.to(torch.double),optical_disk_labels.to(device).to(torch.double)))/100\n",
        "            loss0 = self.criterion(F.log_softmax(retinopathy_pred.double(), dim = -1), F.softmax(retinopathy_label.to(device).double(), dim = -1)).double()\n",
        "            loss1 = self.criterion(F.log_softmax(macular_edema_pred.double(), dim = -1), F.softmax(macular_edema_label.to(device).double(), dim = -1)).double()\n",
        "            loss2 = torch.sqrt(self.criterion2(fovea_center_pred.to(torch.double),fovea_center_labels.to(device).to(torch.double)))\n",
        "            loss3 = torch.sqrt(self.criterion2(optical_disk_pred.to(torch.double),optical_disk_labels.to(device).to(torch.double)))\n",
        "            loss0sum += loss0\n",
        "            loss1sum += loss1\n",
        "            loss2sum += loss2\n",
        "            loss3sum += loss3\n",
        "            #pred0 = F.softmax(retinopathy_pred, dim = -1)\n",
        "            #accuracy0 += pred0.eq(retinopathy_label.to(device)).sum().item()\n",
        "            #pred1 = F.softmax(macular_edema_pred, dim = -1)\n",
        "            #accuracy1 += pred1.eq(macular_edema_label.to(device)).sum().item()\n",
        "            loss = torch.stack((loss0, loss1, loss2 ,loss3))[self.tasks].sum()\n",
        "            if i%4==0:\n",
        "              print('=',end=\"\")\n",
        "            #print('Batch number: {}\\nLoss on batch: {}\\nLoss0: {}\\nLoss1: {}\\nLoss2: {}\\nLoss3: {}\\n-----------------------'.format(i,loss, loss0, loss1, loss2 ,loss3))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            train_loss += loss\n",
        "        print(\"\\nTotal Loss: {}\\nLoss0: {} \\nLoss1: {}  \\nLoss2: {}\\nLoss3: {}\".format(train_loss, loss0sum,  loss1sum, loss2sum,loss3sum))\n",
        "        return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KGBFTSmfAloo"
      },
      "outputs": [],
      "source": [
        "rx=450\n",
        "ry=300\n",
        "old_x=4288\n",
        "old_y=2848\n",
        "Rx=rx/old_x\n",
        "Ry=ry/old_y\n",
        "\n",
        "data_transformer = transforms.Compose([transforms.Resize((rx,ry)),transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "#train_ds = IDRiD_Dataset(data_transformer,'train')\n",
        "train_ds = IDRiD_Dataset_teacher(data_transformer,'train')\n",
        "train_dl = DataLoader(train_ds,batch_size=32,shuffle=True)\n",
        "mtl = MTL()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    mtl=mtl.to(device)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "optimizer = torch.optim.SGD(mtl.parameters(),\n",
        "                                weight_decay=1e-6,\n",
        "                                momentum=0.9,\n",
        "                                lr=1e-3,\n",
        "                                nesterov=True)\n",
        "scheduler = ReduceLROnPlateau(optimizer,\n",
        "                                  factor=0.5,\n",
        "                                  patience=3,\n",
        "                                  min_lr=1e-7,\n",
        "                                  verbose=True)\n",
        "tasks=[[0, 2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pTB7N4tAloq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3e3ae9-b539-4dac-94fc-e1a3815fb98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 ====\n",
            "Total Loss: 22247.480112208854\n",
            "Loss0: 1.8731121332646599 \n",
            "Loss1: 20.261356773058917  \n",
            "Loss2: 22245.60700007559\n",
            "Loss3: 2.133712227079205\n",
            "Saved best model weights!\n",
            "Epoch: 2 ====\n",
            "Total Loss: 22183.761801705503\n",
            "Loss0: 0.7245313184360394 \n",
            "Loss1: 20.076574852816897  \n",
            "Loss2: 22183.03727038707\n",
            "Loss3: 2.2492384317624357\n",
            "Saved best model weights!\n",
            "Epoch: 3 ====\n",
            "Total Loss: 21930.695201163533\n",
            "Loss0: 0.5718495879084434 \n",
            "Loss1: 19.595548132770553  \n",
            "Loss2: 21930.12335157563\n",
            "Loss3: 3.2562535216771407\n",
            "Saved best model weights!\n",
            "Epoch: 4 ====\n",
            "Total Loss: 11897.0936817935\n",
            "Loss0: 0.6859996577485539 \n",
            "Loss1: 19.476056831402392  \n",
            "Loss2: 11896.407682135752\n",
            "Loss3: 9.856515499289152\n",
            "Saved best model weights!\n",
            "Epoch: 5 ====\n",
            "Total Loss: 5106.793331470044\n",
            "Loss0: 0.2750804566299502 \n",
            "Loss1: 19.23254784323991  \n",
            "Loss2: 5106.518251013414\n",
            "Loss3: 11.317517125982087\n",
            "Saved best model weights!\n",
            "Epoch: 6 ====\n",
            "Total Loss: 4702.693777569209\n",
            "Loss0: 0.24860471290937988 \n",
            "Loss1: 19.247082297902615  \n",
            "Loss2: 4702.445172856299\n",
            "Loss3: 11.29023194036106\n",
            "Saved best model weights!\n",
            "Epoch: 7 ====\n",
            "Total Loss: 4841.555334262865\n",
            "Loss0: 0.25270378654612413 \n",
            "Loss1: 19.30636850237498  \n",
            "Loss2: 4841.302630476318\n",
            "Loss3: 11.49164048388399\n",
            "Epoch: 8 ====\n",
            "Total Loss: 4498.321017902046\n",
            "Loss0: 0.23753999153126965 \n",
            "Loss1: 19.227411738771494  \n",
            "Loss2: 4498.083477910515\n",
            "Loss3: 11.39709518002572\n",
            "Saved best model weights!\n",
            "Epoch: 9 ====\n",
            "Total Loss: 4460.1461325905475\n",
            "Loss0: 0.2395215481708763 \n",
            "Loss1: 19.267956284736382  \n",
            "Loss2: 4459.906611042377\n",
            "Loss3: 11.442945553984819\n",
            "Saved best model weights!\n",
            "Epoch: 10 ====\n",
            "Total Loss: 4421.224382979134\n",
            "Loss0: 0.23857036833582165 \n",
            "Loss1: 19.235983191322678  \n",
            "Loss2: 4420.985812610797\n",
            "Loss3: 11.380601102329617\n",
            "Saved best model weights!\n",
            "Epoch: 11 ====\n",
            "Total Loss: 4489.453145122888\n",
            "Loss0: 0.23182614711136257 \n",
            "Loss1: 19.232166212604305  \n",
            "Loss2: 4489.221318975777\n",
            "Loss3: 11.331341005050707\n",
            "Epoch: 12 ====\n",
            "Total Loss: 4507.618558038974\n",
            "Loss0: 0.22973940728298298 \n",
            "Loss1: 19.23987294871053  \n",
            "Loss2: 4507.388818631691\n",
            "Loss3: 11.273385415550422\n",
            "Epoch: 13 ====\n",
            "Total Loss: 4470.033971133042\n",
            "Loss0: 0.23739257069994066 \n",
            "Loss1: 19.22489145412083  \n",
            "Loss2: 4469.796578562342\n",
            "Loss3: 11.265002661817938\n",
            "Epoch: 14 ====\n",
            "Total Loss: 4540.378096588935\n",
            "Loss0: 0.24203695293955463 \n",
            "Loss1: 19.21105506422638  \n",
            "Loss2: 4540.1360596359955\n",
            "Loss3: 11.215115401997485\n",
            "Epoch: 15 ====\n",
            "Total Loss: 4510.029857212796\n",
            "Loss0: 0.2388294579699219 \n",
            "Loss1: 19.236685987808862  \n",
            "Loss2: 4509.791027754827\n",
            "Loss3: 11.268346832924628\n",
            "Epoch: 16 ====\n",
            "Total Loss: 4570.026245474338\n",
            "Loss0: 0.23164532933709636 \n",
            "Loss1: 19.191653090337773  \n",
            "Loss2: 4569.794600145\n",
            "Loss3: 11.243592841231369\n",
            "Epoch: 17 ====\n",
            "Total Loss: 4475.088346959068\n",
            "Loss0: 0.23457288658288483 \n",
            "Loss1: 19.267697400895194  \n",
            "Loss2: 4474.853774072486\n",
            "Loss3: 11.30119787554033\n",
            "Epoch: 18 ====\n",
            "Total Loss: 4526.053008391918\n",
            "Loss0: 0.2385706900700429 \n",
            "Loss1: 19.231974183481686  \n",
            "Loss2: 4525.814437701848\n",
            "Loss3: 11.271412396821322\n",
            "Epoch: 19 ====\n",
            "Total Loss: 4499.165995714705\n",
            "Loss0: 0.23627905055127701 \n",
            "Loss1: 19.22265805615304  \n",
            "Loss2: 4498.929716664153\n",
            "Loss3: 11.270058483254832\n",
            "Epoch: 20 ====\n",
            "Total Loss: 4536.196012955678\n",
            "Loss0: 0.2350242579570298 \n",
            "Loss1: 19.197149899934715  \n",
            "Loss2: 4535.960988697721\n",
            "Loss3: 11.273688862355243\n",
            "Epoch: 21 ====\n",
            "Total Loss: 4463.95956793795\n",
            "Loss0: 0.24500487560796846 \n",
            "Loss1: 19.203857575539693  \n",
            "Loss2: 4463.714563062342\n",
            "Loss3: 11.225395663575735\n",
            "Epoch: 22 ====\n",
            "Total Loss: 4411.293017203552\n",
            "Loss0: 0.23334003246759177 \n",
            "Loss1: 19.18551279524898  \n",
            "Loss2: 4411.059677171086\n",
            "Loss3: 11.208774380740962\n",
            "Saved best model weights!\n",
            "Epoch: 23 ====\n",
            "Total Loss: 4732.28738346227\n",
            "Loss0: 0.24005942155481041 \n",
            "Loss1: 19.197974940850834  \n",
            "Loss2: 4732.047324040715\n",
            "Loss3: 11.146151457121718\n",
            "Epoch: 24 ====\n",
            "Total Loss: 4558.251618135974\n",
            "Loss0: 0.24074555518449042 \n",
            "Loss1: 19.29051348407527  \n",
            "Loss2: 4558.0108725807895\n",
            "Loss3: 11.159440752735689\n",
            "Epoch: 25 ====\n",
            "Total Loss: 4469.494507948639\n",
            "Loss0: 0.23916238388310512 \n",
            "Loss1: 19.1676590140036  \n",
            "Loss2: 4469.255345564755\n",
            "Loss3: 11.136637828010626\n",
            "Epoch: 26 ====\n",
            "Total Loss: 4423.381524376254\n",
            "Loss0: 0.23471083074833812 \n",
            "Loss1: 19.189388243936  \n",
            "Loss2: 4423.146813545505\n",
            "Loss3: 11.254020065405594\n",
            "Epoch: 27 ====\n",
            "Total Loss: 4471.99946890887\n",
            "Loss0: 0.2382501222549238 \n",
            "Loss1: 19.24719685028242  \n",
            "Loss2: 4471.761218786615\n",
            "Loss3: 11.304970250848514\n",
            "Epoch: 28 ====\n",
            "Total Loss: 4346.715135364579\n",
            "Loss0: 0.2327802701603987 \n",
            "Loss1: 19.212502257243745  \n",
            "Loss2: 4346.482355094418\n",
            "Loss3: 11.224978893935274\n",
            "Saved best model weights!\n",
            "Epoch: 29 ====\n",
            "Total Loss: 4550.993008534591\n",
            "Loss0: 0.23919967504011913 \n",
            "Loss1: 19.215726608556768  \n",
            "Loss2: 4550.753808859551\n",
            "Loss3: 11.141899474041043\n",
            "Epoch: 30 ====\n",
            "Total Loss: 4537.92763655204\n",
            "Loss0: 0.23510830017925644 \n",
            "Loss1: 19.18335335079135  \n",
            "Loss2: 4537.6925282518605\n",
            "Loss3: 11.118939545446533\n",
            "Epoch: 31 ====\n",
            "Total Loss: 4440.463437816263\n",
            "Loss0: 0.23286906499236407 \n",
            "Loss1: 19.184084140842238  \n",
            "Loss2: 4440.230568751271\n",
            "Loss3: 11.02755286079285\n",
            "Epoch: 32 ====\n",
            "Total Loss: 4515.950785603011\n",
            "Loss0: 0.2416135848807003 \n",
            "Loss1: 19.172682133847747  \n",
            "Loss2: 4515.709172018131\n",
            "Loss3: 11.03373916707842\n",
            "Epoch: 33 ====\n",
            "Total Loss: 4470.902770121547\n",
            "Loss0: 0.2376768942400934 \n",
            "Loss1: 19.19100154384226  \n",
            "Loss2: 4470.665093227307\n",
            "Loss3: 11.00497013421743\n",
            "Epoch: 34 ====\n",
            "Total Loss: 4535.779264409638\n",
            "Loss0: 0.24263198070298514 \n",
            "Loss1: 19.162545713833286  \n",
            "Loss2: 4535.536632428934\n",
            "Loss3: 10.95643464315522\n",
            "Epoch: 35 ====\n",
            "Total Loss: 4436.062324310234\n",
            "Loss0: 0.23809496726154647 \n",
            "Loss1: 19.18793376467115  \n",
            "Loss2: 4435.824229342971\n",
            "Loss3: 10.959053086185419\n",
            "Epoch: 36 ====\n",
            "Total Loss: 4507.970665515257\n",
            "Loss0: 0.234970438686532 \n",
            "Loss1: 19.180408014609743  \n",
            "Loss2: 4507.735695076571\n",
            "Loss3: 10.92344004187462\n",
            "Epoch: 37 ====\n",
            "Total Loss: 4548.926113176653\n",
            "Loss0: 0.23644002297344574 \n",
            "Loss1: 19.164799763550114  \n",
            "Loss2: 4548.689673153681\n",
            "Loss3: 10.85509822679019\n",
            "Epoch: 38 ====\n",
            "Total Loss: 4433.521015898893\n",
            "Loss0: 0.2408048483671274 \n",
            "Loss1: 19.165456169429195  \n",
            "Loss2: 4433.280211050526\n",
            "Loss3: 10.81606827475493\n",
            "Epoch: 39 ====\n",
            "Total Loss: 4404.588708194935\n",
            "Loss0: 0.23559677335806303 \n",
            "Loss1: 19.194444542515775  \n",
            "Loss2: 4404.353111421577\n",
            "Loss3: 10.79394798347577\n",
            "Epoch: 40 ====\n",
            "Total Loss: 4358.276552291747\n",
            "Loss0: 0.2367730745647341 \n",
            "Loss1: 19.13658702191458  \n",
            "Loss2: 4358.0397792171825\n",
            "Loss3: 10.752853533546881\n",
            "Epoch: 41 ====\n",
            "Total Loss: 4373.278752472672\n",
            "Loss0: 0.23909124582186245 \n",
            "Loss1: 19.162577836042363  \n",
            "Loss2: 4373.0396612268505\n",
            "Loss3: 10.726630774807228\n",
            "Epoch: 42 ====\n",
            "Total Loss: 4443.439303081694\n",
            "Loss0: 0.23597483060474575 \n",
            "Loss1: 19.1567821357542  \n",
            "Loss2: 4443.203328251088\n",
            "Loss3: 10.668203117704785\n",
            "Epoch: 43 ====\n",
            "Total Loss: 4465.473693069138\n",
            "Loss0: 0.24005698654215632 \n",
            "Loss1: 19.15212228931633  \n",
            "Loss2: 4465.233636082597\n",
            "Loss3: 10.691636046460914\n",
            "Epoch: 44 ====\n",
            "Total Loss: 4424.090517345559\n",
            "Loss0: 0.23922962356145736 \n",
            "Loss1: 19.10876241061012  \n",
            "Loss2: 4423.851287721998\n",
            "Loss3: 10.678958328963992\n",
            "Epoch: 45 ====\n",
            "Total Loss: 4444.768040748734\n",
            "Loss0: 0.23478556045736074 \n",
            "Loss1: 19.135391749470944  \n",
            "Loss2: 4444.533255188277\n",
            "Loss3: 10.682913808981358\n",
            "Epoch: 46 ====\n",
            "Total Loss: 4391.671916286037\n",
            "Loss0: 0.23690248133695438 \n",
            "Loss1: 19.137086530239085  \n",
            "Loss2: 4391.435013804699\n",
            "Loss3: 10.676208457311409\n",
            "Epoch: 47 ====\n",
            "Total Loss: 4406.815494849725\n",
            "Loss0: 0.24323844917148696 \n",
            "Loss1: 19.09414465587754  \n",
            "Loss2: 4406.572256400554\n",
            "Loss3: 10.629353886386863\n",
            "Epoch: 48 ====\n",
            "Total Loss: 4377.321049332157\n",
            "Loss0: 0.2337497334577257 \n",
            "Loss1: 19.113340954768535  \n",
            "Loss2: 4377.0872995987\n",
            "Loss3: 10.608301753718653\n",
            "Epoch: 49 ====\n",
            "Total Loss: 4391.572239665216\n",
            "Loss0: 0.23164781453006827 \n",
            "Loss1: 19.119033626144365  \n",
            "Loss2: 4391.340591850686\n",
            "Loss3: 10.589331244645557\n",
            "Epoch: 50 ====\n",
            "Total Loss: 4413.3174369967\n",
            "Loss0: 0.23304407245560776 \n",
            "Loss1: 19.064717354937713  \n",
            "Loss2: 4413.084392924244\n",
            "Loss3: 10.565692445695976\n",
            "Epoch: 51 ====\n",
            "Total Loss: 4397.090484494084\n",
            "Loss0: 0.23115963176232904 \n",
            "Loss1: 19.097090384832683  \n",
            "Loss2: 4396.859324862321\n",
            "Loss3: 10.565439438220544\n",
            "Epoch: 52 ====\n",
            "Total Loss: 4407.936901336055\n",
            "Loss0: 0.23546001819579435 \n",
            "Loss1: 19.102232714542946  \n",
            "Loss2: 4407.701441317859\n",
            "Loss3: 10.467679053535646\n",
            "Epoch: 53 ====\n",
            "Total Loss: 4455.812248011206\n",
            "Loss0: 0.23533108181808524 \n",
            "Loss1: 19.118631457118838  \n",
            "Loss2: 4455.576916929387\n",
            "Loss3: 10.478543053971213\n",
            "Epoch: 54 ====\n",
            "Total Loss: 4490.794748472921\n",
            "Loss0: 0.23895548818821813 \n",
            "Loss1: 19.06076367593146  \n",
            "Loss2: 4490.555792984733\n",
            "Loss3: 10.452480798508924\n",
            "Epoch: 55 ====\n",
            "Total Loss: 4339.727341396086\n",
            "Loss0: 0.22948002043056748 \n",
            "Loss1: 19.113060742788015  \n",
            "Loss2: 4339.497861375655\n",
            "Loss3: 10.447719185341287\n",
            "Saved best model weights!\n",
            "Epoch: 56 ====\n",
            "Total Loss: 4314.676808021246\n",
            "Loss0: 0.24452567902757844 \n",
            "Loss1: 19.111948039582973  \n",
            "Loss2: 4314.432282342219\n",
            "Loss3: 10.41098103334044\n",
            "Saved best model weights!\n",
            "Epoch: 57 ====\n",
            "Total Loss: 4314.863678369191\n",
            "Loss0: 0.2314296239180223 \n",
            "Loss1: 19.142173304434372  \n",
            "Loss2: 4314.632248745274\n",
            "Loss3: 10.413892321640411\n",
            "Epoch: 58 ====\n",
            "Total Loss: 4600.873358709366\n",
            "Loss0: 0.23195286255409528 \n",
            "Loss1: 19.088916070390894  \n",
            "Loss2: 4600.641405846813\n",
            "Loss3: 10.361728527024663\n",
            "Epoch: 59 ====\n",
            "Total Loss: 4339.96939059838\n",
            "Loss0: 0.2321929063305853 \n",
            "Loss1: 19.08973536619952  \n",
            "Loss2: 4339.73719769205\n",
            "Loss3: 10.3165652768813\n",
            "Epoch: 60 ====\n",
            "Total Loss: 4363.419361817613\n",
            "Loss0: 0.2393570004826606 \n",
            "Loss1: 19.108191886438007  \n",
            "Loss2: 4363.180004817131\n",
            "Loss3: 10.313006202208983\n",
            "Epoch: 61 ====\n",
            "Total Loss: 4264.269395104779\n",
            "Loss0: 0.2297632124502743 \n",
            "Loss1: 19.099193500295424  \n",
            "Loss2: 4264.0396318923285\n",
            "Loss3: 10.250623065848423\n",
            "Saved best model weights!\n",
            "Epoch: 62 ====\n",
            "Total Loss: 4415.127862443727\n",
            "Loss0: 0.23365246639519124 \n",
            "Loss1: 19.069908622143004  \n",
            "Loss2: 4414.894209977331\n",
            "Loss3: 10.190272971830005\n",
            "Epoch: 63 ====\n",
            "Total Loss: 4463.866658501412\n",
            "Loss0: 0.23404714243619196 \n",
            "Loss1: 19.046010967891547  \n",
            "Loss2: 4463.632611358976\n",
            "Loss3: 10.20094028965273\n",
            "Epoch: 64 ====\n",
            "Total Loss: 4543.286731653985\n",
            "Loss0: 0.2393676546330013 \n",
            "Loss1: 19.10386734894144  \n",
            "Loss2: 4543.0473639993525\n",
            "Loss3: 10.202572367471614\n",
            "Epoch: 65 ====\n",
            "Total Loss: 4348.843478590142\n",
            "Loss0: 0.23422781811106605 \n",
            "Loss1: 19.096894651042877  \n",
            "Loss2: 4348.6092507720305\n",
            "Loss3: 10.199849790497478\n",
            "Epoch: 66 ====\n",
            "Total Loss: 4404.93860756416\n",
            "Loss0: 0.23299266369210228 \n",
            "Loss1: 19.110190314996466  \n",
            "Loss2: 4404.705614900468\n",
            "Loss3: 10.137536835460462\n",
            "Epoch: 67 ====\n",
            "Total Loss: 4369.070803012637\n",
            "Loss0: 0.2330496267629137 \n",
            "Loss1: 19.03023855268949  \n",
            "Loss2: 4368.837753385875\n",
            "Loss3: 10.16063874092301\n",
            "Epoch: 68 ====\n",
            "Total Loss: 4402.9738061831595\n",
            "Loss0: 0.23448224584481203 \n",
            "Loss1: 19.071478808223596  \n",
            "Loss2: 4402.739323937315\n",
            "Loss3: 10.128288091610177\n",
            "Epoch: 69 ====\n",
            "Total Loss: 4358.153889066234\n",
            "Loss0: 0.23663031390386213 \n",
            "Loss1: 19.08652281348831  \n",
            "Loss2: 4357.917258752331\n",
            "Loss3: 10.136877565873364\n",
            "Epoch: 70 ====\n",
            "Total Loss: 4295.554313977353\n",
            "Loss0: 0.23472142614207667 \n",
            "Loss1: 19.082626925354685  \n",
            "Loss2: 4295.319592551211\n",
            "Loss3: 10.133661789321513\n",
            "Epoch: 71 ====\n",
            "Total Loss: 4366.417113013432\n",
            "Loss0: 0.2311598666426294 \n",
            "Loss1: 19.067250429746608  \n",
            "Loss2: 4366.18595314679\n",
            "Loss3: 10.133171923459756\n",
            "Epoch: 72 ====\n",
            "Total Loss: 4344.2647160048045\n",
            "Loss0: 0.2353420482635546 \n",
            "Loss1: 19.06228737736386  \n",
            "Loss2: 4344.029373956541\n",
            "Loss3: 10.096042526584913\n",
            "Epoch: 73 ====\n",
            "Total Loss: 4471.781997357559\n",
            "Loss0: 0.2328294493961594 \n",
            "Loss1: 19.10344025865763  \n",
            "Loss2: 4471.549167908163\n",
            "Loss3: 10.13779974334324\n",
            "Epoch: 74 =="
          ]
        }
      ],
      "source": [
        "mtl.fit(train_dl,optimizer,scheduler,criterion,tasks,30000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.listdir(\"/content/drive/MyDrive/Warsztaty badawcze\")"
      ],
      "metadata": {
        "id": "vFaWMM6HKNbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "mtl=MTL()\n",
        "mtl.load_state_dict(torch.load(\"/content/drive/MyDrive/Warsztaty badawcze/weights2.pt\"))\n",
        "mtl.eval()\n",
        "data=pd.DataFrame()\n",
        "z = []\n",
        "for i, (imgs, retinopathy_label, macular_edema_label, fovea_center_labels, optical_disk_labels) in enumerate(train_ds):\n",
        "    one_row = mtl.forward(imgs[None,:])\n",
        "    z.append(torch.concat(one_row).detach().numpy())\n",
        "data = pd.DataFrame(z)\n",
        "data.to_csv('teacher_train_predictions.csv')"
      ],
      "metadata": {
        "id": "bNfRBbnNNE09"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Teacher_prediction.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "483aeb2f258bd2a173809688f12c66e6038f5b29dd5fc23ebacdcb77cdae07a9"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}