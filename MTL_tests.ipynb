{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from dataset import IDRiD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTL, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.features = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
    "        self.last = nn.Sequential(nn.Linear(2048, 1024),nn.ReLU())\n",
    "        self.retinopathy_classifier = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 5), nn.Softmax(dim=1))\n",
    "        self.macular_edema_classifier = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 5), nn.Softmax(dim=1))\n",
    "        self.fovea_center_cords = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 2))\n",
    "        self.optical_disk_cords = nn.Sequential(nn.Linear(1024, 512),nn.ReLU(), nn.Linear(512, 2))\n",
    "\n",
    "    def forward(self,data):\n",
    "        out = self.features.forward(data).squeeze()\n",
    "        out = self.last.forward(out)\n",
    "        return (self.retinopathy_classifier(out),\n",
    "                self.macular_edema_classifier(out),\n",
    "                self.fovea_center_cords(out),\n",
    "                self.optical_disk_cords(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50x25 just for implementing\n",
    "data_transformer = transforms.Compose([transforms.Resize((500,250)),transforms.ToTensor()])\n",
    "train_ds = IDRiD_Dataset(data_transformer,'train')\n",
    "train_dl = DataLoader(train_ds,batch_size=2,shuffle=True)\n",
    "mtl = MTL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_trainer():\n",
    "    def __init__(self,mtl,optimizer, scheduler, criterion, tasks, epochs):\n",
    "        self.mtl = mtl\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.criterion2 = nn.MSELoss()\n",
    "        self.tasks = tasks\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self,train_dl):\n",
    "        for e in range(self.epochs):\n",
    "            self.mtl.train()\n",
    "            print(self.train_iter(train_dl).item())\n",
    "            \n",
    "\n",
    "    def train_iter(self, train_dl):\n",
    "        train_loss = 0.0\n",
    "        loss = torch.tensor(0)\n",
    "        for i, (imgs, retinopathy_label, macular_edema_label, fovea_center_labels, optical_disk_labels) in enumerate(train_dl):\n",
    "            batch_size = imgs.size(0)\n",
    "            self.optimizer.zero_grad()\n",
    "            retinopathy_pred, macular_edema_pred, fovea_center_pred, optical_disk_pred = self.mtl(imgs)\n",
    "            loss0 = self.criterion(retinopathy_pred, retinopathy_label.to(torch.int64)).to(torch.float64)*100\n",
    "            loss1 = self.criterion(macular_edema_pred, macular_edema_label.to(torch.int64)).to(torch.float64)*100\n",
    "            loss2 = torch.sqrt(self.criterion2(fovea_center_pred.to(torch.double),fovea_center_labels.to(torch.double)))/10\n",
    "            loss3 = torch.sqrt(self.criterion2(optical_disk_pred.to(torch.double),optical_disk_labels.to(torch.double)))/10\n",
    "            loss = torch.stack((loss0, loss1, loss2 ,loss3))[self.tasks].sum()\n",
    "            print('Batch number: {}\\nLoss on batch: {}\\nLoss0: {}\\nLoss1: {}\\nLoss2: {}\\nLoss3: {}\\n-----------------------'.format(i,loss.item(), loss0.item(), loss1.item(), loss2.item() ,loss3.item()))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mtl.parameters(),\n",
    "                                weight_decay=1e-6,\n",
    "                                momentum=0.9,\n",
    "                                lr=1e-3,\n",
    "                                nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                  factor=0.5,\n",
    "                                  patience=3,\n",
    "                                  min_lr=1e-7,\n",
    "                                  verbose=True)\n",
    "tasks=[[0,1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 0\n",
      "Loss on batch: 7.241564384506855\n",
      "Loss0: 1.5996904373168945\n",
      "Loss1: 1.6031171083450317\n",
      "Loss2: 1.618451106433125\n",
      "Loss3: 2.4203057324118036\n",
      "-----------------------\n",
      "Batch number: 1\n",
      "Loss on batch: 6.905180373988713\n",
      "Loss0: 1.6074074506759644\n",
      "Loss1: 1.6019704341888428\n",
      "Loss2: 1.8358105584608195\n",
      "Loss3: 1.8599919306630874\n",
      "-----------------------\n",
      "Batch number: 2\n",
      "Loss on batch: 6.8507506300268535\n",
      "Loss0: 1.6038973331451416\n",
      "Loss1: 1.6029586791992188\n",
      "Loss2: 1.7924470261231553\n",
      "Loss3: 1.8514475915593374\n",
      "-----------------------\n",
      "Batch number: 3\n",
      "Loss on batch: 7.163377940470573\n",
      "Loss0: 1.6049728393554688\n",
      "Loss1: 1.6015524864196777\n",
      "Loss2: 1.6077287260215134\n",
      "Loss3: 2.3491238886739136\n",
      "-----------------------\n",
      "Batch number: 4\n",
      "Loss on batch: 7.236367794161778\n",
      "Loss0: 1.6024727821350098\n",
      "Loss1: 1.5963181257247925\n",
      "Loss2: 1.6711564361399855\n",
      "Loss3: 2.3664204501619905\n",
      "-----------------------\n",
      "Batch number: 5\n",
      "Loss on batch: 6.903582478417452\n",
      "Loss0: 1.5949654579162598\n",
      "Loss1: 1.5937516689300537\n",
      "Loss2: 1.7877951120866513\n",
      "Loss3: 1.9270702394844876\n",
      "-----------------------\n",
      "Batch number: 6\n",
      "Loss on batch: 6.80332390970988\n",
      "Loss0: 1.6014719009399414\n",
      "Loss1: 1.5997549295425415\n",
      "Loss2: 1.7811678810951888\n",
      "Loss3: 1.8209291981322084\n",
      "-----------------------\n",
      "Batch number: 7\n",
      "Loss on batch: 6.730514681312197\n",
      "Loss0: 1.6061863899230957\n",
      "Loss1: 1.5936306715011597\n",
      "Loss2: 1.6939057375835826\n",
      "Loss3: 1.8367918823043594\n",
      "-----------------------\n",
      "Batch number: 8\n",
      "Loss on batch: 7.075578452839329\n",
      "Loss0: 1.6070384979248047\n",
      "Loss1: 1.596231460571289\n",
      "Loss2: 1.868741492639838\n",
      "Loss3: 2.003567001703397\n",
      "-----------------------\n",
      "Batch number: 9\n",
      "Loss on batch: 6.805022557026081\n",
      "Loss0: 1.6148815155029297\n",
      "Loss1: 1.5926597118377686\n",
      "Loss2: 1.7698625112738442\n",
      "Loss3: 1.8276188184115385\n",
      "-----------------------\n",
      "Batch number: 10\n",
      "Loss on batch: 6.635195750255268\n",
      "Loss0: 1.6309435367584229\n",
      "Loss1: 1.600076675415039\n",
      "Loss2: 1.6341900813973373\n",
      "Loss3: 1.769985456684469\n",
      "-----------------------\n",
      "Batch number: 11\n",
      "Loss on batch: 6.306043162346175\n",
      "Loss0: 1.6012225151062012\n",
      "Loss1: 1.591407060623169\n",
      "Loss2: 1.909286469921338\n",
      "Loss3: 1.2041271166954666\n",
      "-----------------------\n",
      "Batch number: 12\n",
      "Loss on batch: 7.1211298807341326\n",
      "Loss0: 1.5984965562820435\n",
      "Loss1: 1.5884394645690918\n",
      "Loss2: 1.9234507948285866\n",
      "Loss3: 2.0107430650544105\n",
      "-----------------------\n",
      "Batch number: 13\n",
      "Loss on batch: 6.280994008908446\n",
      "Loss0: 1.6115686893463135\n",
      "Loss1: 1.5747878551483154\n",
      "Loss2: 1.9171348541880393\n",
      "Loss3: 1.1775026102257782\n",
      "-----------------------\n",
      "Batch number: 14\n",
      "Loss on batch: 6.198034519693791\n",
      "Loss0: 1.604349970817566\n",
      "Loss1: 1.5938055515289307\n",
      "Loss2: 1.8561988368478073\n",
      "Loss3: 1.1436801604994875\n",
      "-----------------------\n",
      "Batch number: 15\n",
      "Loss on batch: 6.738230656566396\n",
      "Loss0: 1.603235125541687\n",
      "Loss1: 1.592674732208252\n",
      "Loss2: 1.7943440146935496\n",
      "Loss3: 1.7479767841229066\n",
      "-----------------------\n",
      "Batch number: 16\n",
      "Loss on batch: 6.181539769074153\n",
      "Loss0: 1.6104507446289062\n",
      "Loss1: 1.578213095664978\n",
      "Loss2: 1.860508230924822\n",
      "Loss3: 1.1323676978554464\n",
      "-----------------------\n",
      "Batch number: 17\n",
      "Loss on batch: 6.737427615611959\n",
      "Loss0: 1.5966953039169312\n",
      "Loss1: 1.5874028205871582\n",
      "Loss2: 1.7837298576888696\n",
      "Loss3: 1.7695996334190003\n",
      "-----------------------\n",
      "Batch number: 18\n",
      "Loss on batch: 6.846277682924633\n",
      "Loss0: 1.6012202501296997\n",
      "Loss1: 1.6379787921905518\n",
      "Loss2: 1.7832435614861473\n",
      "Loss3: 1.8238350791182345\n",
      "-----------------------\n",
      "Batch number: 19\n",
      "Loss on batch: 6.337389789455869\n",
      "Loss0: 1.5922200679779053\n",
      "Loss1: 1.5807640552520752\n",
      "Loss2: 1.9710909212637098\n",
      "Loss3: 1.1933147449621782\n",
      "-----------------------\n",
      "Batch number: 20\n",
      "Loss on batch: 6.947951855146122\n",
      "Loss0: 1.5985374450683594\n",
      "Loss1: 1.5644221305847168\n",
      "Loss2: 1.8213132599899815\n",
      "Loss3: 1.9636790195030642\n",
      "-----------------------\n",
      "Batch number: 21\n",
      "Loss on batch: 6.207212518674553\n",
      "Loss0: 1.6007059812545776\n",
      "Loss1: 1.567617416381836\n",
      "Loss2: 1.8860088784886628\n",
      "Loss3: 1.1528802425494769\n",
      "-----------------------\n",
      "Batch number: 22\n",
      "Loss on batch: 6.805914219472699\n",
      "Loss0: 1.5914015769958496\n",
      "Loss1: 1.563347578048706\n",
      "Loss2: 1.7918841863420785\n",
      "Loss3: 1.8592808780860648\n",
      "-----------------------\n",
      "Batch number: 23\n",
      "Loss on batch: 6.978843178779471\n",
      "Loss0: 1.5998746156692505\n",
      "Loss1: 1.5597167015075684\n",
      "Loss2: 1.8103550120820187\n",
      "Loss3: 2.008896849520633\n",
      "-----------------------\n",
      "Batch number: 24\n",
      "Loss on batch: 6.827243214659232\n",
      "Loss0: 1.617093563079834\n",
      "Loss1: 1.5679504871368408\n",
      "Loss2: 1.7886256421484081\n",
      "Loss3: 1.8535735222941492\n",
      "-----------------------\n",
      "Batch number: 25\n",
      "Loss on batch: 6.081006306139926\n",
      "Loss0: 1.6110267639160156\n",
      "Loss1: 1.5637234449386597\n",
      "Loss2: 1.865348631487327\n",
      "Loss3: 1.0409074657979236\n",
      "-----------------------\n",
      "Batch number: 26\n",
      "Loss on batch: 6.865411538082533\n",
      "Loss0: 1.6011981964111328\n",
      "Loss1: 1.5559909343719482\n",
      "Loss2: 1.4913837429242538\n",
      "Loss3: 2.2168386643751985\n",
      "-----------------------\n",
      "Batch number: 27\n",
      "Loss on batch: 6.652280823743964\n",
      "Loss0: 1.604303240776062\n",
      "Loss1: 1.5569050312042236\n",
      "Loss2: 1.7353394105176532\n",
      "Loss3: 1.7557331412460249\n",
      "-----------------------\n",
      "Batch number: 28\n",
      "Loss on batch: 6.33901383667422\n",
      "Loss0: 1.5913643836975098\n",
      "Loss1: 1.5535123348236084\n",
      "Loss2: 1.9952857103818862\n",
      "Loss3: 1.1988514077712165\n",
      "-----------------------\n",
      "Batch number: 29\n",
      "Loss on batch: 6.950082728021652\n",
      "Loss0: 1.5895376205444336\n",
      "Loss1: 1.5500085353851318\n",
      "Loss2: 1.8962864829443256\n",
      "Loss3: 1.9142500891477607\n",
      "-----------------------\n",
      "Batch number: 30\n",
      "Loss on batch: 6.945559155730846\n",
      "Loss0: 1.6253817081451416\n",
      "Loss1: 1.5477327108383179\n",
      "Loss2: 1.531077227629627\n",
      "Loss3: 2.24136750911776\n",
      "-----------------------\n",
      "Batch number: 31\n",
      "Loss on batch: 6.842890880877299\n",
      "Loss0: 1.582279086112976\n",
      "Loss1: 1.597076654434204\n",
      "Loss2: 1.8064611592458997\n",
      "Loss3: 1.857073981084219\n",
      "-----------------------\n",
      "Batch number: 32\n",
      "Loss on batch: 6.613414637334746\n",
      "Loss0: 1.586077094078064\n",
      "Loss1: 1.5437853336334229\n",
      "Loss2: 1.690365277599045\n",
      "Loss3: 1.7931869320242144\n",
      "-----------------------\n",
      "Batch number: 33\n",
      "Loss on batch: 7.409598703415548\n",
      "Loss0: 1.6197283267974854\n",
      "Loss1: 1.5514169931411743\n",
      "Loss2: 1.7584996969977884\n",
      "Loss3: 2.4799536864791003\n",
      "-----------------------\n",
      "Batch number: 34\n",
      "Loss on batch: 6.3495874152218645\n",
      "Loss0: 1.619946837425232\n",
      "Loss1: 1.548691987991333\n",
      "Loss2: 1.9791052733309473\n",
      "Loss3: 1.201843316474352\n",
      "-----------------------\n",
      "Batch number: 35\n",
      "Loss on batch: 6.609089923692133\n",
      "Loss0: 1.6214439868927002\n",
      "Loss1: 1.6008570194244385\n",
      "Loss2: 1.5954255428957076\n",
      "Loss3: 1.791363374479287\n",
      "-----------------------\n",
      "Batch number: 36\n",
      "Loss on batch: 6.655111318038852\n",
      "Loss0: 1.6185407638549805\n",
      "Loss1: 1.604053020477295\n",
      "Loss2: 1.6922537223460898\n",
      "Loss3: 1.7402638113604874\n",
      "-----------------------\n",
      "Batch number: 37\n",
      "Loss on batch: 6.778936001643597\n",
      "Loss0: 1.5629150867462158\n",
      "Loss1: 1.5308027267456055\n",
      "Loss2: 1.8110916045593641\n",
      "Loss3: 1.8741265835924126\n",
      "-----------------------\n",
      "Batch number: 38\n",
      "Loss on batch: 6.281554314768098\n",
      "Loss0: 1.580789566040039\n",
      "Loss1: 1.5301976203918457\n",
      "Loss2: 1.986793486585145\n",
      "Loss3: 1.1837736417510678\n",
      "-----------------------\n",
      "Batch number: 39\n",
      "Loss on batch: 6.9851867394279745\n",
      "Loss0: 1.6016478538513184\n",
      "Loss1: 1.596590518951416\n",
      "Loss2: 1.576951448750711\n",
      "Loss3: 2.2099969178745296\n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-c59fb0a18570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmtl_trainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMTL_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmtl_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-871d14438750>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dl)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-871d14438750>\u001b[0m in \u001b[0;36mtrain_iter\u001b[1;34m(self, train_dl)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss2\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mloss3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Batch number: {}\\nLoss on batch: {}\\nLoss0: {}\\nLoss1: {}\\nLoss2: {}\\nLoss3: {}\\n-----------------------'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mloss3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mtl_trainer=MTL_trainer(mtl,optimizer,scheduler,criterion,tasks,2)\n",
    "mtl_trainer.train(train_dl)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483aeb2f258bd2a173809688f12c66e6038f5b29dd5fc23ebacdcb77cdae07a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
